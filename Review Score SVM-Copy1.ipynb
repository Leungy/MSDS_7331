{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Member Names:\n",
    "\n",
    "- Name 1: Jasmine Coleman\n",
    "- Name 2: Yat Leung\n",
    "- Name 3: Karen Somes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will an AirBnb host receive a perfect rating?\n",
    "### Classifying \"perfect\" hosts using logistic regression and SVM ###\n",
    "\n",
    "__Cleaning the data__  \n",
    "First, we removed variables with high correlation, repetitive values, and attributes with a high number of missing values to narrow our focus of predictors and performed data transformations to aid our analysis.  \n",
    "    \n",
    "Our target classification variable is a Boolean indicator for whether or not a host received a \"perfect\" 100 review score. Through our classification and analysis, we will determine which attributes are most powerfully associated with a 100 rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jazis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (61,62,94,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from decimal import Decimal\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "data = pd.read_csv(\"/Users/jazis/Downloads/listings.csv\")\n",
    "\n",
    "#data cleaning from LAB1\n",
    "#drop redundant info and fields not useful for analysis\n",
    "sub=data.drop(['id','listing_url','scrape_id','last_scraped','summary','space','description','experiences_offered'\n",
    "              , 'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'house_rules',\n",
    "              'thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url', 'host_url', 'host_thumbnail_url',\n",
    "              'host_picture_url', 'country_code', 'country','amenities', 'minimum_minimum_nights',\n",
    "              'maximum_minimum_nights','minimum_maximum_nights', 'maximum_maximum_nights','minimum_nights_avg_ntm',\n",
    "              'maximum_nights_avg_ntm', 'availability_30', 'availability_365','availability_90','has_availability',\n",
    "               'calculated_host_listings_count','calculated_host_listings_count_shared_rooms',\n",
    "               'is_business_travel_ready','host_about', 'host_acceptance_rate', 'host_total_listings_count',\n",
    "              'jurisdiction_names','license','monthly_price','square_feet','weekly_price', 'requires_license'], axis=1)\n",
    "def money_to_decimal(x):\n",
    "    x = x.replace(\"$\", \"\").replace(\",\", \"\").replace(\" \", \"\")\n",
    "    return float(x)\n",
    "def rem_percent(x):\n",
    "    x=x.replace(\"%\",\"\")\n",
    "    return float(x)/100\n",
    "def truncate(n):\n",
    "    return int(n * 1000) / 1000\n",
    "#converts objects with money values into decimal values to become continous attribute\n",
    "sub.cleaning_fee = sub.cleaning_fee.astype(str)\n",
    "sub.extra_people = sub.extra_people.astype(str)\n",
    "sub.security_deposit = sub.security_deposit.astype(str)\n",
    "sub.price = sub.price.astype(str)\n",
    "sub.loc[:,'price'] = sub.loc[:,'price'].apply(money_to_decimal)\n",
    "sub.loc[:,'cleaning_fee'] = sub.loc[:,'cleaning_fee'].apply(money_to_decimal)\n",
    "sub.loc[:,'extra_people'] = sub.loc[:,'extra_people'].apply(money_to_decimal)\n",
    "sub.loc[:,'security_deposit'] = sub.loc[:,'security_deposit'].apply(money_to_decimal)\n",
    "sub.host_response_rate = sub.host_response_rate.astype(str)\n",
    "sub.loc[:,'host_response_rate'] = sub.loc[:, 'host_response_rate'].apply(rem_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating dummy variables for SVM__\n",
    "\n",
    "We created dummy variables for the categorical variables that we would include in the model. We iteratively determined which categorical variables to include by checking the accuracy of each interim model when removing terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jazis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\jazis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\jazis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jazis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jazis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\jazis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "df = sub[~sub['review_scores_rating'].isnull()]\n",
    "df['perf_score'] = np.where(df['review_scores_rating']==100, 1, 0)\n",
    "\n",
    "df_data=df\n",
    "df_y=df['perf_score']\n",
    "\n",
    "#create dummy vars\n",
    "##host_loc = pd.get_dummies(df_data['host_location'],drop_first=True)\n",
    "host_response = pd.get_dummies(df_data['host_response_time'],drop_first=True)\n",
    "##host_neigh = pd.get_dummies(df_data['neighbourhood_group_cleansed'],drop_first=True)\n",
    "##host_verif = pd.get_dummies(df_data['host_verifications'],drop_first=True)\n",
    "df_data['host_identity_verified'] = pd.get_dummies(df_data['host_identity_verified'],drop_first=True)\n",
    "##street = pd.get_dummies(df_data['street'],drop_first=True)\n",
    "neighborhood = pd.get_dummies(df_data['neighbourhood_group_cleansed'],drop_first=True)\n",
    "##city = pd.get_dummies(df_data['city'],drop_first=True)\n",
    "# make into continuous zipcode = pd.get_dummies(x_train['zipcode'],drop_first=True)\n",
    "##market = pd.get_dummies(df_data['market'],drop_first=True)\n",
    "df_data['is_location_exact'] = pd.get_dummies(df_data['is_location_exact'],drop_first=True)\n",
    "prop_type = pd.get_dummies(df_data['property_type'],drop_first=True)\n",
    "room_type = pd.get_dummies(df_data['room_type'],drop_first=True)\n",
    "bed_type = pd.get_dummies(df_data['bed_type'],drop_first=True)\n",
    "df_data['instant_bookable'] = pd.get_dummies(df_data['instant_bookable'],drop_first=True)\n",
    "cancel = pd.get_dummies(df_data['cancellation_policy'],drop_first=True)\n",
    "df_data['host_is_superhost'] = pd.get_dummies(df_data['host_is_superhost'],drop_first=True)\n",
    "\n",
    "df_data.drop(['host_location','host_response_time','host_neighbourhood','host_verifications',\n",
    "             'street', 'neighbourhood', 'city', 'market', \n",
    "             'property_type', 'room_type', 'bed_type', 'instant_bookable',\n",
    "             'cancellation_policy', 'name', 'host_name', 'host_has_profile_pic', 'neighbourhood_cleansed',\n",
    "              'neighbourhood_group_cleansed', 'host_neighbourhood', 'smart_location', 'calendar_updated',\n",
    "             'calendar_last_scraped','require_guest_profile_picture', 'require_guest_phone_verification',\n",
    "             'host_since', 'first_review', 'last_review', 'state', 'smart_location', 'zipcode','review_scores_rating'],axis=1,inplace=True)\n",
    "\n",
    "df_data = pd.concat([df_data, host_response, prop_type, room_type,\n",
    "                    bed_type, cancel, neighborhood],axis=1)\n",
    "\n",
    "##x_train, x_test, y_train, y_test = train_test_split(df_data, df_y, test_size=0.2)\n",
    "#ent_r = entropy_value(ds.target[feature28>2.5])\n",
    "#ent_l = entropy_value(ds.target[feature28<=2.5])\n",
    "#ent_t = entropy_value(feature28)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fitting the SVM__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "##splitting prection from predictor variables\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "\n",
    "if 'perf_score' in df_data:\n",
    "    y = df_data['perf_score'].values # get the labels we want\n",
    "    del df_data['perf_score'] # get rid of the class label\n",
    "    X = df_data.values # use everything else to predict!\n",
    "\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "\n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting training numpy to pandas df to inpute nan values\n",
    "cols = list(df_data.columns.values)\n",
    "impute = pd.DataFrame(X_train, columns=cols)\n",
    "#impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting test numpy to pandas df to inpute nan values\n",
    "cols = list(df_data.columns.values)\n",
    "impute_test = pd.DataFrame(X_test, columns=cols)\n",
    "#impute_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputations - training\n",
    "impute['price']=impute.price.mask(impute.price == 0,impute.price.median())\n",
    "impute.cleaning_fee=impute.cleaning_fee.fillna(impute.cleaning_fee.median())\n",
    "impute.host_response_rate=impute.host_response_rate.fillna(impute.host_response_rate.median())\n",
    "impute.review_scores_accuracy=impute.review_scores_accuracy.fillna(truncate(impute.review_scores_accuracy.median()))\n",
    "impute.review_scores_checkin=impute.review_scores_checkin.fillna(truncate(impute.review_scores_checkin.median()))\n",
    "impute.review_scores_cleanliness=impute.review_scores_cleanliness.fillna(truncate(impute.review_scores_cleanliness.median()))\n",
    "impute.review_scores_communication=impute.review_scores_communication.fillna(truncate(impute.review_scores_communication.median()))\n",
    "impute.review_scores_location=impute.review_scores_location.fillna(truncate(impute.review_scores_location.median()))\n",
    "#sub.review_scores_rating=sub.review_scores_rating.fillna(truncate(sub.review_scores_rating.median()))\n",
    "impute.review_scores_value=impute.review_scores_value.fillna(truncate(impute.review_scores_value.median()))\n",
    "impute.reviews_per_month=impute.reviews_per_month.fillna(impute.reviews_per_month.median())\n",
    "impute.security_deposit=impute.security_deposit.fillna(impute.security_deposit.median())\n",
    "impute.bathrooms=impute.bathrooms.fillna(impute.bathrooms.median())\n",
    "impute.bedrooms=impute.bedrooms.fillna(impute.bedrooms.median())\n",
    "impute.host_listings_count=impute.host_listings_count.fillna(impute.host_listings_count.median())\n",
    "impute.beds=impute.beds.fillna(impute.beds.median())\n",
    "#sub.host_response_time=sub.host_response_time.fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputations - test\n",
    "impute_test['price']=impute_test.price.mask(impute.price == 0,impute.price.median())\n",
    "impute_test.cleaning_fee=impute_test.cleaning_fee.fillna(impute.cleaning_fee.median())\n",
    "impute_test.host_response_rate=impute_test.host_response_rate.fillna(impute.host_response_rate.median())\n",
    "impute_test.review_scores_accuracy=impute_test.review_scores_accuracy.fillna(truncate(impute.review_scores_accuracy.median()))\n",
    "impute_test.review_scores_checkin=impute_test.review_scores_checkin.fillna(truncate(impute.review_scores_checkin.median()))\n",
    "impute_test.review_scores_cleanliness=impute_test.review_scores_cleanliness.fillna(truncate(impute.review_scores_cleanliness.median()))\n",
    "impute_test.review_scores_communication=impute_test.review_scores_communication.fillna(truncate(impute.review_scores_communication.median()))\n",
    "impute_test.review_scores_location=impute_test.review_scores_location.fillna(truncate(impute.review_scores_location.median()))\n",
    "#sub.review_scores_rating=sub.review_scores_rating.fillna(truncate(sub.review_scores_rating.median()))\n",
    "impute_test.review_scores_value=impute_test.review_scores_value.fillna(truncate(impute.review_scores_value.median()))\n",
    "impute_test.reviews_per_month=impute_test.reviews_per_month.fillna(impute.reviews_per_month.median())\n",
    "impute_test.security_deposit=impute_test.security_deposit.fillna(impute.security_deposit.median())\n",
    "impute_test.bathrooms=impute_test.bathrooms.fillna(impute.bathrooms.median())\n",
    "impute_test.bedrooms=impute_test.bedrooms.fillna(impute.bedrooms.median())\n",
    "impute_test.host_listings_count=impute_test.host_listings_count.fillna(impute.host_listings_count.median())\n",
    "impute_test.beds=impute_test.beds.fillna(impute.beds.median())\n",
    "#sub.host_response_time=sub.host_response_time.fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting pandas df back to numpy matrix - for train\n",
    "X_train = impute.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting pandas df back to numpy matrix - for test\n",
    "X_test = impute_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jazis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy=\"median\", axis=0)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "##took longer to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVM Performance__  \n",
    "The support vector machine outperformed the logistic model with 84% accuracy. The logistic model was only able to predict non-perfect scores -- in which the majority of reviews fall. However, we are more interested in being able to accurately predict when a customer will leave a perfect review which occurs far less frequently. This logistic model was able to identify both with 91% accuracy identifying true-negatives (less than perfect scores) and 68% accuracy identifying true-positives (perfect scores).\n",
    "\n",
    "__SVM Advantage__  \n",
    "This model outperforms the logistic model in terms of accuracy. Although this model does take longer to run than the logistic model, because this dataset is not that large, the extra few seconds it takes to run is not a big disadvantage. This classification model is also advantageous because SVMs tend to perform better with higher dimensional models, are less sensitive to outliers, and are less computationally intensive. This is important because we include several categorical variables that have numerous unique levels (ranging from 2 to 222) in our predictive model that have been broken out into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the support vectors\n",
    "print(svm_clf.support_vectors_.shape) #(observations, variables) to be support vectors\n",
    "print(svm_clf.support_.shape) #(observations,) on the edge of vector\n",
    "print(svm_clf.n_support_ ) #([observations on the edge for one class/ obs on edge for the other class ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grab the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = df_data.iloc[train_indices] # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "\n",
    "df_support['perf_score'] = y[svm_clf.support_] # add back in the 'perf_score' column to the pandas dataframe\n",
    "df_data['perf_score'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['perf_score'])\n",
    "df_grouped = df_data.groupby(['perf_score'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['host_id',\n",
    "'host_response_rate',\n",
    "'host_is_superhost',\n",
    "'host_listings_count',\n",
    "'host_identity_verified',\n",
    "'latitude',\n",
    "'longitude',\n",
    "'is_location_exact',\n",
    "'accommodates',\n",
    "'bathrooms',\n",
    "'bedrooms',\n",
    "'beds',\n",
    "'price',\n",
    "'security_deposit',\n",
    "'cleaning_fee',\n",
    "'guests_included',\n",
    "'extra_people',\n",
    "'minimum_nights',\n",
    "'maximum_nights',\n",
    "'availability_60',\n",
    "'number_of_reviews',\n",
    "'number_of_reviews_ltm',\n",
    "'review_scores_accuracy',\n",
    "'review_scores_cleanliness',\n",
    "'review_scores_checkin',\n",
    "'review_scores_communication',\n",
    "'review_scores_location',\n",
    "'review_scores_value',\n",
    "'calculated_host_listings_count_entire_homes',\n",
    "'calculated_host_listings_count_private_rooms',\n",
    "'reviews_per_month',\n",
    "'within a day',\n",
    "'within a few hours',\n",
    "'within an hour',\n",
    "'Apartment',\n",
    "'Bed and breakfast',\n",
    "'Boat',\n",
    "'Boutique hotel',\n",
    "'Bungalow',\n",
    "'Cabin',\n",
    "'Camper/RV',\n",
    "'Casa particular (Cuba)',\n",
    "'Castle',\n",
    "'Cave',\n",
    "'Condominium',\n",
    "'Cottage',\n",
    "'Dome house',\n",
    "'Earth house',\n",
    "'Guest suite',\n",
    "'Guesthouse',\n",
    "'Hostel',\n",
    "'Hotel',\n",
    "'House',\n",
    "'Houseboat',\n",
    "'Lighthouse',\n",
    "'Loft',\n",
    "'Nature lodge',\n",
    "'Other',\n",
    "'Resort',\n",
    "'Serviced apartment',\n",
    "'Tent',\n",
    "'Tiny house',\n",
    "'Townhouse',\n",
    "'Treehouse',\n",
    "'Villa',\n",
    "'Private room',\n",
    "'Shared room',\n",
    "'Couch',\n",
    "'Futon',\n",
    "'Pull-out Sofa',\n",
    "'Real Bed',\n",
    "'moderate',\n",
    "'strict',\n",
    "'strict_14_with_grace_period',\n",
    "'super_strict_30',\n",
    "'super_strict_60',\n",
    "'Brooklyn',\n",
    "'Manhattan',\n",
    "'Queens',\n",
    "'Staten Island']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Not Perfect','Perfect'])\n",
    "    plt.title(v+' (SV Instances)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Not Perfect','Perfect'])\n",
    "    plt.title(v+' (Original)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVM Chosen Support Vectors__  \n",
    "The model identified ~13000 records in the dataset across 80 variables that were on the border of being misclassified which created 80 support vectors. These vectors include variables pertaining to the cancellation policy, location (neighborhood and coordinates), accommodations (property type, bed type, rooms, etc.), host characteristics (id, response time, verified, number of listings, etc.), prices and fees and other rating categories. The support vectors range to include many different types of variables, but there is no specific category that stands out. However, many of the support vectors are from levels of a categorical variable that had very small volumes (like castle, caves, pull-out sofa) or many unique values (like host id or price). It makes sense that the model would have a harder time predicting a classification class when there is very little training data available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
